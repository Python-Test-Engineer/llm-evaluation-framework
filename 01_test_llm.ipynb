{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466c4606",
   "metadata": {},
   "source": [
    "<img src=\"./images/01-takeaways.png\" width=700px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-N9PZf8...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from a file called .env\n",
    "# There is a sample file .env.sample - rename this to .env and put your API keys there.\n",
    "# https://console.groq.com/login has a free tier that uses the same interface as OpenAI\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:14]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a90e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selected: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "print(f\"Model selected: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949f018",
   "metadata": {},
   "source": [
    "For demo, we will create our own OpenAI request class but in future we will use the OpenAI library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6ba59",
   "metadata": {},
   "source": [
    "\n",
    "Temperature - The OpenAI API incorporates a hyperparameter known as temperature that affects the computation of token probabilities when generating output through the large language model. The temperature value ranges from 0 to 2, with lower values indicating greater determinism and higher values indicating more randomness. Default is 1.\n",
    "\n",
    "Low Temperature:\n",
    "\n",
    "The bag is full of mostly blue marbles, with a few red and green. Low temperature means you're very likely to pull a blue marble, but you might occasionally get a red or green.\n",
    "\n",
    "High Temperature:\n",
    "\n",
    "The bag is filled with a mix of colors, and all colors are equally likely. High temperature means you're equally likely to pull any color, including the less common ones.\n",
    "\n",
    "<img src=\"./images/temperature.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1226148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially making a POST request to one endpoint and we create a convenience Class to pass in prompts, temperature and model.\n",
    "\n",
    "\n",
    "class MyCustomOpenAI:\n",
    "\n",
    "    def __init__(self, system_prompt=None, temperature=1.0, model=MODEL):\n",
    "        self.model_endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.api_key = openai_api_key\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "        }\n",
    "\n",
    "    # Note that other than the API key to be authenticated, we pass nor reference to previous queries. Each request is STATELESS and is independent of any other request.\n",
    "\n",
    "    # That is why we append previous output to new queries so that the LLM has the full context or 'picture'.\n",
    "\n",
    "    # OpenAI was trained with 'messages' in the form of a list of messages - SYSTEM, USER, ASSISTANT - so using this is most effective.\n",
    "\n",
    "    # SYSTEM will contain the 'character' and instruction set of the Agent, USER will be the input to the Agent and ASSISTANT will be the output.\n",
    "\n",
    "    # TAKEAWAY: WWe are appending the last output to the list of 'messages' so that the Agent has knowledge of the context - the history of the conversation.\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"stream\": False,  # no streaming or output\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        # Use HTTP POST method from Requests library\n",
    "\n",
    "        # In future examples we will use OpenAI library instead of requests directly but this is for demo purposes.\n",
    "        response = requests.post(\n",
    "            url=self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "        ).json()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548bb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of MyCustomOpenAI and pass in a request - just a convenience wrapper of making a REST API call.\n",
    "\n",
    "# System Prompt sets up the character of the AI Agent and can be considered to be the API route we are creating and it it will be in Natural Language - explained later...\n",
    "\n",
    "client = MyCustomOpenAI(\n",
    "    model=MODEL,\n",
    "    system_prompt=\"You give concise answers to questions with no more than 200 characters\",\n",
    "    temperature=0.0,  # as deterministic as possible\n",
    ")\n",
    "\n",
    "response = client.generate_text(\n",
    "    \"What is an the difference between Data Science, Data Engineering and Data Analysis?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39feb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Incorrect API key provided: sk-proj-**********************************************************************************************************************************************************************************************************************************************************DuYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We can drill down into 'response' to get our choice of data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# The 'answer' is in the message with 'role': 'assistant'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "# We can drill down into 'response' to get our choice of data\n",
    "# The 'answer' is in the message with 'role': 'assistant'\n",
    "print(response)\n",
    "print(response[\"choices\"][0])\n",
    "print(response[\"choices\"][0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data Science focuses on extracting insights from data using statistical '\n",
      " 'methods. Data Engineering involves building and maintaining data '\n",
      " 'infrastructure. Data Analysis interprets data to inform decisions.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8ee55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
